= Use Studio to Configure Hadoop (HDFS) Connector - Mule 4
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

Anypoint Studio (Studio) editors help you design and update your Mule applications, properties, and configuration files.

To configure a connector in Studio:

. Create a Mule project.
. Supply your authentication credentials to Anypoint Platform.
. Add the connector to your Mule project
. Configure an input source for the connector's flow.
. Configure the connector.

Follow these best practices when you configure an input source and connector in Studio:

* Configure an associated global element for the input source and connector.
* Create a YAML or properties file to contain your properties, and then reference the file from the configuration.
* Configure a reconnection strategy for the input source and target connector.

== Add the Connector to Your Project

Add Hadoop (HDFS) Connector to a Mule project to automatically populate the XML code with the connector's namespace and schema location, as well as add the required dependencies to the project's `pom.xml` file.

. In Studio, create a Mule project.
. In the Mule Palette view, click *(X) Search in Exchange*.
. In *Add Modules to Project*, type HDFS in the search field.
. Click the Hadoop (HDFS) connector in *Available modules*.
. Click *Add*.
. Click *Finish*.

When you design your application in Studio, the act of dragging the connector's operation from the palette onto the Anypoint Studio canvas automatically populates the XML code with the connector namespace and schema location.

Adding a connector to a Mule project in Studio does not make that connector available to other projects in your Studio workspace.

== Configure an Input Source

An input source initiates a flow when a specified condition is met. Possible input source operations for this connector are:

* Read +
Start your app when the content of a file is read and streamed to the rest of the flow.
* HTTP Listener +
Start your app using a browser or a command such as `curl`.
* Scheduler +
Start your app at timed intervals.

In Studio, drag the input operation you choose from the *Mule Palette* to the *Source* panel in the canvas.

The required fields for the `Read` input source are:

* Configuration +
The name of the connector configuration to use
* Path +
The path for the file to read 

[[configure_global_element]]
== Configure a Global Element for the Connector

To configure a connector in your Mule app, first configure a global element that can be used by all instances of that connector in the flow:

. In the Mule Palette view, search for the connector and select an operation.
. Drag the operation onto the canvas to the right of the input source. 
. In the configuration screen for the operation, click the plus sign (+) next to the *Connector configuration* field to add a global element for the connector.
. In the *General* tab, enter a name for the configuration, and select the connection type from *Connection*. +
Supported connection types for this connector are: 
* *Kerberos*
* *Simple*
+
For information about how to configure the connections, see <<configure_authentication,Configure Authentication>>.
. Click the *Advanced* tab to specify a reconnection strategy, if supported.
. Click *Test Connection* to confirm that Mule can connect with the specified server.
. Click *OK*.

[[configure_authentication]]
== Configure Authentication

To access the data in a Hadoop (HDFS) instance, you must authenticate your applicationâ€™s requests using a supported authentication method. You can select the authentication method when you <<<configure_global_element,configure a global element>> for the connector. 

For Hadoop (HDFS) Connector, the following connection authentication types are supported:

* <<kerberos,Kerberos>>
* <<hdfs_simple,Simple>>

[[kerberos]]
=== Kerberos

[%header,cols="20s,20a,35a,20a,5a"]
|===
| Name | Type | Description | Default Value | Required
| Username a| String |  The Kerberos principal. The Username is passed to the HDFS client as the `hadoop.job.ugi` configuration entry. The Username can be overriden by values you specify in *Configuration Resources* and *Configuration Entries*. This parameter is called Username for backward compatibility reasons. |  | 
| Keytab Path a| String | Enter the path to the https://web.mit.edu/kerberos/krb5-1.12/doc/basic/keytab_def.html[keytab file] associated with the Username you specified. The path is used to obtain a ticket granting ticket (TGT) from the authorization server. If this value is not provided Kerberos looks for a TGT associated with the specified Username within your local Kerberos cache. |  | 
| Name Node Uri a| String |  The name of the file system to connect to. The Name Node is passed to the HDFS client as the `FileSystem#FS_DEFAULT_NAME_KEY` configuration entry. The Name Node can be overriden by values you specify in *Configuration Resources* and *Configuration Entries*. |  | x
| Configuration Resources a| Array of String |  A `java.util.List` of configuration resource files for the HDFS client to load. You can provide additional configuration files, for example, `core-site.xml`. |  | 
| Configuration Entries a| Object |  A `java.util.Map` of configuration entries to use by the HDFS client. You can provide additional configuration entries as key-value pairs. |  | 
| Reconnection a| <<Reconnection>> |  When the application is deployed, a connectivity test is performed on all connectors. If reconnection is enabled, deployment fails if the test doesn't pass after exhausting the associated reconnection strategy. |  | 
|===

[[hdfs_simple]]
=== Simple


[%header,cols="20s,20a,35a,20a,5a"]
|===
| Name | Type | Description | Default Value | Required
| Username a| String |  User identity that Hadoop uses for permissions in HDFS. When Simple authentication is used, Hadoop requires that the user is set as a system property called `HADOOP_USER_NAME`. If a value is not provided, Hadoop uses the username of the OS user who is currently logged in. |  | 
| Name Node Uri a| String |  The name of the file system to which to connect. The Name Node is passed to the HDFS client as the `FileSystem#FS_DEFAULT_NAME_KEY` configuration entry. The Name Node can be overriden by values you specify in *Configuration Resources* and *Configuration Entries*. |  | x
| Configuration Resources a| Array of String |  A `java.util.List` of configuration resource files for the HDFS client to load. You can provide additional configuration files, for example, `core-site.xml`. |  | 
| Configuration Entries a| Object |  A `java.util.Map` of configuration entries for the HDFS client to use. You can provide additional configuration entries as key-value pairs. |  | 
| Reconnection a| <<Reconnection>> |  When the application is deployed, a connectivity test is performed on all connectors. If reconnection is enabled, deployment fails if the test doesn't pass after exhausting the associated reconnection strategy. |  | 
|===


== See Also

* xref:introduction/anypoint-connector-authentication.adoc[Anypoint Connector Authentication]
// Add link to connector reference and specific connector intro topic
* https://help.mulesoft.com[MuleSoft Help Center]
